/**
 * Enhanced Multimodal AI Service - Unified Orchestration Layer
 * Delegates to existing WAI SDK services: ComputerVisionAPI, VoiceSynthesisEngine, DocumentParsingService
 * Provides a unified API for vision, speech, and document processing
 */

import { EventEmitter } from 'events';
import crypto from 'crypto';
import { ComputerVisionAPI, type CVAnalysisRequest, type CVAnalysisResult } from './computer-vision-api';
import { VoiceSynthesisEngine } from './voice-synthesis-engine';
import { DocumentParsingService } from './document-parsing-service';
import fs from 'fs/promises';
import path from 'path';
import os from 'os';

// Types
export interface VisionAnalysisRequest {
  imageData?: string; // base64
  imageUrl?: string;
  imagePath?: string;
  prompt?: string;
  analysisType: 'describe' | 'ocr' | 'objects' | 'faces' | 'scene' | 'custom';
  language?: string;
  options?: {
    detail?: 'low' | 'high' | 'auto';
    maxTokens?: number;
    includeConfidence?: boolean;
  };
}

export interface VisionAnalysisResult {
  id: string;
  analysisType: string;
  result: {
    description?: string;
    objects?: Array<{ name: string; confidence: number; location?: string }>;
    text?: string;
    faces?: Array<{ description: string; emotion?: string; age?: string }>;
    scene?: { description: string; categories: string[]; mood?: string };
    custom?: string;
  };
  metadata: {
    model: string;
    processingTime: number;
    tokensUsed: number;
    timestamp: string;
  };
}

export interface SpeechToTextRequest {
  audioData?: string; // base64
  audioPath?: string;
  audioUrl?: string;
  language?: string;
  prompt?: string;
  responseFormat?: 'json' | 'text' | 'srt' | 'vtt' | 'verbose_json';
  model?: 'whisper-1';
  options?: {
    temperature?: number;
    timestamps?: boolean;
  };
}

export interface SpeechToTextResult {
  id: string;
  text: string;
  language?: string;
  duration?: number;
  segments?: Array<{
    id: number;
    start: number;
    end: number;
    text: string;
    confidence?: number;
  }>;
  words?: Array<{
    word: string;
    start: number;
    end: number;
  }>;
  metadata: {
    model: string;
    processingTime: number;
    timestamp: string;
  };
}

export interface TextToSpeechRequest {
  text: string;
  voice?: 'alloy' | 'echo' | 'fable' | 'onyx' | 'nova' | 'shimmer';
  model?: 'tts-1' | 'tts-1-hd';
  speed?: number; // 0.25 to 4.0
  responseFormat?: 'mp3' | 'opus' | 'aac' | 'flac' | 'wav' | 'pcm';
  options?: {
    elevenlabs?: boolean;
    elevenLabsVoiceId?: string;
    elevenLabsSettings?: {
      stability?: number;
      similarityBoost?: number;
      style?: number;
    };
  };
}

export interface TextToSpeechResult {
  id: string;
  audioBuffer: Buffer;
  audioBase64: string;
  format: string;
  duration?: number;
  metadata: {
    model: string;
    voice: string;
    processingTime: number;
    timestamp: string;
    provider: 'openai' | 'elevenlabs' | 'browser-tts' | 'system-tts';
  };
}

export interface DocumentProcessingRequest {
  documentData?: string; // base64
  documentPath?: string;
  documentUrl?: string;
  documentType: 'pdf' | 'image' | 'text' | 'office';
  operations: Array<'extract_text' | 'summarize' | 'translate' | 'analyze' | 'qa'>;
  userId?: string;
  options?: {
    language?: string;
    targetLanguage?: string;
    question?: string;
    summaryLength?: 'brief' | 'detailed';
    includeMetadata?: boolean;
  };
}

export interface DocumentProcessingResult {
  id: string;
  documentType: string;
  results: {
    extractedText?: string;
    summary?: string;
    translation?: string;
    analysis?: {
      topics: string[];
      entities: string[];
      sentiment: string;
      keyPoints: string[];
    };
    answer?: string;
  };
  metadata: {
    pageCount?: number;
    wordCount?: number;
    processingTime: number;
    timestamp: string;
  };
}

export interface MultimodalConversationMessage {
  role: 'user' | 'assistant' | 'system';
  content: string | Array<{
    type: 'text' | 'image_url' | 'audio';
    text?: string;
    image_url?: { url: string; detail?: string };
    audio?: { data: string; format: string };
  }>;
}

export interface MultimodalConversationRequest {
  messages: MultimodalConversationMessage[];
  model?: string;
  maxTokens?: number;
  temperature?: number;
  stream?: boolean;
}

/**
 * Enhanced Multimodal AI Service - Unified Orchestration Layer
 * Properly delegates to existing WAI SDK services
 */
export class EnhancedMultimodalService extends EventEmitter {
  private computerVisionApi: ComputerVisionAPI;
  private voiceSynthesisEngine: VoiceSynthesisEngine;
  private documentParsingService: DocumentParsingService;
  private cache = new Map<string, any>();
  private readonly maxCacheSize = 500;
  private processingQueue = new Map<string, Promise<any>>();

  constructor() {
    super();
    this.computerVisionApi = new ComputerVisionAPI();
    this.voiceSynthesisEngine = new VoiceSynthesisEngine();
    this.documentParsingService = new DocumentParsingService();
    console.log('ðŸŽ¨ Enhanced Multimodal AI Service initialized');
    console.log('   âœ… Delegating to ComputerVisionAPI for vision analysis');
    console.log('   âœ… Delegating to VoiceSynthesisEngine for speech synthesis');
    console.log('   âœ… Delegating to DocumentParsingService for document processing');
  }

  // ==================== Vision Analysis (Delegates to ComputerVisionAPI) ====================

  async analyzeVision(request: VisionAnalysisRequest): Promise<VisionAnalysisResult> {
    const startTime = Date.now();
    const analysisId = crypto.randomUUID();

    try {
      // Map request to ComputerVisionAPI format
      const cvRequest: CVAnalysisRequest = {
        imageData: request.imageData,
        imageUrl: request.imageUrl,
        imagePath: request.imagePath,
        analysisTypes: {
          objectDetection: request.analysisType === 'objects',
          faceDetection: request.analysisType === 'faces',
          textExtraction: request.analysisType === 'ocr',
          sceneAnalysis: request.analysisType === 'scene' || request.analysisType === 'describe',
        },
        options: {
          customPrompt: request.prompt,
          language: request.language,
          generateTags: true,
        },
      };

      // Delegate to ComputerVisionAPI
      const cvResult = await this.computerVisionApi.analyze(cvRequest);

      // Transform result to unified format
      const result: VisionAnalysisResult = {
        id: analysisId,
        analysisType: request.analysisType,
        result: {
          description: cvResult.analysis.scene?.description,
          objects: cvResult.analysis.objects?.map(obj => ({
            name: obj.label,
            confidence: obj.confidence,
            location: obj.boundingBox ? `${obj.boundingBox.x},${obj.boundingBox.y}` : undefined,
          })),
          text: cvResult.analysis.text?.fullText,
          faces: cvResult.analysis.faces?.map(face => ({
            description: `Face detected with ${Math.round(face.confidence * 100)}% confidence`,
            emotion: face.attributes?.emotions 
              ? Object.entries(face.attributes.emotions).sort((a, b) => b[1] - a[1])[0]?.[0]
              : undefined,
            age: face.attributes?.age?.toString(),
          })),
          scene: cvResult.analysis.scene ? {
            description: cvResult.analysis.scene.description,
            categories: cvResult.analysis.scene.categories,
            mood: undefined,
          } : undefined,
        },
        metadata: {
          model: cvResult.metadata.model,
          processingTime: Date.now() - startTime,
          tokensUsed: 0,
          timestamp: new Date().toISOString(),
        },
      };

      this.emit('visionAnalysis', result);
      return result;
    } catch (error) {
      console.error('Vision analysis error:', error);
      throw error;
    }
  }

  // ==================== Speech-to-Text (Uses WAI SDK transcription) ====================

  async transcribeSpeech(request: SpeechToTextRequest): Promise<SpeechToTextResult> {
    const startTime = Date.now();
    const transcriptionId = crypto.randomUUID();
    let tempPath: string | null = null;

    try {
      let audioBuffer: Buffer;

      // Get audio data
      if (request.audioData) {
        audioBuffer = Buffer.from(request.audioData, 'base64');
      } else if (request.audioPath) {
        audioBuffer = await fs.readFile(request.audioPath);
      } else if (request.audioUrl) {
        const response = await fetch(request.audioUrl);
        if (!response.ok) {
          throw new Error(`Failed to fetch audio from URL: ${response.status} ${response.statusText}`);
        }
        audioBuffer = Buffer.from(await response.arrayBuffer());
      } else {
        throw new Error('No audio source provided. Please provide audioData, audioPath, or audioUrl.');
      }

      // Validate audio buffer
      if (!audioBuffer || audioBuffer.length === 0) {
        throw new Error('Audio data is empty');
      }

      // Save to temp file for processing
      tempPath = path.join(os.tmpdir(), `audio_${transcriptionId}.wav`);
      await fs.writeFile(tempPath, audioBuffer);

      // Use WAI SDK's voice service for transcription (errors are now propagated)
      const transcriptionResult = await this.voiceSynthesisEngine.transcribeAudio(tempPath, {
        language: request.language,
        timestamps: request.options?.timestamps,
      });

      // Clean up temp file
      await fs.unlink(tempPath).catch(() => {});
      tempPath = null;

      const result: SpeechToTextResult = {
        id: transcriptionId,
        text: transcriptionResult.text || '',
        language: transcriptionResult.language || request.language,
        duration: transcriptionResult.duration,
        segments: transcriptionResult.segments,
        metadata: {
          model: 'whisper-1',
          processingTime: Date.now() - startTime,
          timestamp: new Date().toISOString(),
        },
      };

      this.emit('transcription', result);
      return result;
    } catch (error) {
      // Clean up temp file on error
      if (tempPath) {
        await fs.unlink(tempPath).catch(() => {});
      }
      
      // Re-throw the error to propagate to the route handler
      console.error('Speech transcription error:', error);
      throw error;
    }
  }

  // ==================== Text-to-Speech (Delegates to VoiceSynthesisEngine) ====================

  async synthesizeSpeech(request: TextToSpeechRequest): Promise<TextToSpeechResult> {
    const startTime = Date.now();
    const synthesisId = crypto.randomUUID();

    try {
      // Map voice names to VoiceSynthesisEngine format
      const voiceMap: Record<string, string> = {
        'alloy': 'rachel',
        'echo': 'adam',
        'fable': 'bella',
        'onyx': 'adam',
        'nova': 'bella',
        'shimmer': 'rachel',
      };

      const voice = voiceMap[request.voice || 'alloy'] || 'rachel';

      // Delegate to VoiceSynthesisEngine with fallback chain
      const synthesisResult = await this.voiceSynthesisEngine.synthesizeVoice(request.text, {
        voice,
        stability: request.options?.elevenLabsSettings?.stability || 0.5,
        similarityBoost: request.options?.elevenLabsSettings?.similarityBoost || 0.75,
        style: request.options?.elevenLabsSettings?.style || 0,
        speakerBoost: true,
        speed: request.speed || 1.0,
      });

      const audioBuffer = synthesisResult.audioBuffer || Buffer.alloc(0);

      const result: TextToSpeechResult = {
        id: synthesisId,
        audioBuffer,
        audioBase64: audioBuffer.toString('base64'),
        format: request.responseFormat || 'mp3',
        duration: synthesisResult.duration,
        metadata: {
          model: request.model || 'tts-1',
          voice: request.voice || 'alloy',
          processingTime: Date.now() - startTime,
          timestamp: new Date().toISOString(),
          provider: synthesisResult.provider || 'elevenlabs',
        },
      };

      this.emit('speechSynthesis', result);
      return result;
    } catch (error) {
      console.error('Speech synthesis error:', error);
      throw error;
    }
  }

  // ==================== Document Processing (Delegates to DocumentParsingService) ====================

  async processDocument(request: DocumentProcessingRequest): Promise<DocumentProcessingResult> {
    const startTime = Date.now();
    const processingId = crypto.randomUUID();
    const userId = request.userId || 'system';

    try {
      let documentPath: string;
      let originalFilename = 'document';

      // Get document data
      if (request.documentData) {
        documentPath = path.join(os.tmpdir(), `doc_${processingId}.${this.getExtension(request.documentType)}`);
        await fs.writeFile(documentPath, Buffer.from(request.documentData, 'base64'));
        originalFilename = `document.${this.getExtension(request.documentType)}`;
      } else if (request.documentPath) {
        documentPath = request.documentPath;
        originalFilename = path.basename(documentPath);
      } else if (request.documentUrl) {
        const response = await fetch(request.documentUrl);
        const buffer = Buffer.from(await response.arrayBuffer());
        documentPath = path.join(os.tmpdir(), `doc_${processingId}.${this.getExtension(request.documentType)}`);
        await fs.writeFile(documentPath, buffer);
        originalFilename = path.basename(new URL(request.documentUrl).pathname) || `document.${this.getExtension(request.documentType)}`;
      } else {
        throw new Error('No document source provided');
      }

      let parseResult: any;

      // Delegate to DocumentParsingService based on document type
      switch (request.documentType) {
        case 'pdf':
          parseResult = await this.documentParsingService.parsePDF(documentPath, userId, originalFilename);
          break;
        case 'image':
          parseResult = await this.documentParsingService.parseImage(documentPath, userId, originalFilename);
          break;
        case 'text':
          const textContent = await fs.readFile(documentPath, 'utf-8');
          parseResult = {
            extractedText: textContent,
            wordCount: textContent.split(/\s+/).length,
            status: 'completed',
          };
          break;
        case 'office':
          const ext = path.extname(documentPath).toLowerCase().replace('.', '') || 'docx';
          parseResult = await this.documentParsingService.parseOfficeDocument(documentPath, userId, originalFilename, undefined, ext);
          break;
        default:
          parseResult = await this.documentParsingService.parsePDF(documentPath, userId, originalFilename);
      }

      // Clean up temp file if we created it
      if (request.documentData || request.documentUrl) {
        await fs.unlink(documentPath).catch(() => {});
      }

      const result: DocumentProcessingResult = {
        id: processingId,
        documentType: request.documentType,
        results: {
          extractedText: parseResult.extractedText || parseResult.text,
          summary: request.operations.includes('summarize') ? await this.generateSummary(parseResult.extractedText || parseResult.text, request.options?.summaryLength) : undefined,
        },
        metadata: {
          pageCount: parseResult.pageCount,
          wordCount: parseResult.wordCount,
          processingTime: Date.now() - startTime,
          timestamp: new Date().toISOString(),
        },
      };

      this.emit('documentProcessing', result);
      return result;
    } catch (error) {
      console.error('Document processing error:', error);
      throw error;
    }
  }

  private getExtension(documentType: string): string {
    const extensions: Record<string, string> = {
      'pdf': 'pdf',
      'image': 'png',
      'text': 'txt',
      'office': 'docx',
    };
    return extensions[documentType] || 'bin';
  }

  private async generateSummary(text: string, length?: 'brief' | 'detailed'): Promise<string> {
    if (!text) return '';
    
    const words = text.split(/\s+/);
    const targetLength = length === 'detailed' ? 200 : 100;
    
    if (words.length <= targetLength) {
      return text;
    }
    
    // Simple extractive summary - take first sentences
    const sentences = text.split(/[.!?]+/);
    let summary = '';
    for (const sentence of sentences) {
      if ((summary + sentence).split(/\s+/).length <= targetLength) {
        summary += sentence.trim() + '. ';
      } else {
        break;
      }
    }
    
    return summary.trim() || words.slice(0, targetLength).join(' ') + '...';
  }

  // ==================== Multimodal Conversation (Orchestrates multiple modalities) ====================

  async conductMultimodalConversation(request: MultimodalConversationRequest): Promise<any> {
    const startTime = Date.now();
    const conversationId = crypto.randomUUID();

    try {
      const results: any[] = [];

      // Process each message that may contain multimodal content
      for (const message of request.messages) {
        if (typeof message.content === 'string') {
          results.push({ type: 'text', content: message.content });
        } else if (Array.isArray(message.content)) {
          for (const part of message.content) {
            if (part.type === 'image_url' && part.image_url) {
              // Process image through vision API
              const visionResult = await this.analyzeVision({
                imageUrl: part.image_url.url,
                analysisType: 'describe',
              });
              results.push({ type: 'vision', content: visionResult });
            } else if (part.type === 'audio' && part.audio) {
              // Process audio through transcription
              const transcriptionResult = await this.transcribeSpeech({
                audioData: part.audio.data,
              });
              results.push({ type: 'transcription', content: transcriptionResult });
            } else if (part.type === 'text' && part.text) {
              results.push({ type: 'text', content: part.text });
            }
          }
        }
      }

      return {
        id: conversationId,
        results,
        metadata: {
          processingTime: Date.now() - startTime,
          timestamp: new Date().toISOString(),
        },
      };
    } catch (error) {
      console.error('Multimodal conversation error:', error);
      throw error;
    }
  }

  // ==================== Batch Processing ====================

  async processBatch(requests: Array<{
    type: 'vision' | 'speech' | 'document';
    request: VisionAnalysisRequest | SpeechToTextRequest | TextToSpeechRequest | DocumentProcessingRequest;
  }>): Promise<any[]> {
    const results: any[] = [];

    for (const { type, request } of requests) {
      try {
        let result;
        switch (type) {
          case 'vision':
            result = await this.analyzeVision(request as VisionAnalysisRequest);
            break;
          case 'speech':
            if ('text' in request) {
              result = await this.synthesizeSpeech(request as TextToSpeechRequest);
            } else {
              result = await this.transcribeSpeech(request as SpeechToTextRequest);
            }
            break;
          case 'document':
            result = await this.processDocument(request as DocumentProcessingRequest);
            break;
        }
        results.push({ success: true, result });
      } catch (error) {
        results.push({ success: false, error: (error as Error).message });
      }
    }

    return results;
  }

  // ==================== Service Status ====================

  getServiceStatus(): {
    vision: boolean;
    speech: boolean;
    document: boolean;
    providers: string[];
  } {
    return {
      vision: true,
      speech: true,
      document: true,
      providers: ['computer-vision-api', 'voice-synthesis-engine', 'document-parsing-service'],
    };
  }

  // ==================== Cache Management ====================

  clearCache(): void {
    this.cache.clear();
    console.log('ðŸ§¹ Multimodal service cache cleared');
  }

  getCacheStats(): { size: number; maxSize: number } {
    return {
      size: this.cache.size,
      maxSize: this.maxCacheSize,
    };
  }
}

// Singleton instance
let enhancedMultimodalServiceInstance: EnhancedMultimodalService | null = null;

export function getEnhancedMultimodalService(): EnhancedMultimodalService {
  if (!enhancedMultimodalServiceInstance) {
    enhancedMultimodalServiceInstance = new EnhancedMultimodalService();
  }
  return enhancedMultimodalServiceInstance;
}

export default EnhancedMultimodalService;
